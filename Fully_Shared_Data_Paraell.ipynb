{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wcmaLeDELA6",
        "outputId": "5614e576-7229-43ae-f7ea-49511645bf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tifffile import imread  # Added for TIFF file handling\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler('palm_tree_analysis.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PalmTreeAnalysis:\n",
        "    def __init__(self):\n",
        "        self.tree_model = None\n",
        "        self.health_model = None\n",
        "        logger.info(\"PalmTreeAnalysis initialized\")\n",
        "\n",
        "    def create_synthetic_dataset(self, source_image_path):\n",
        "        \"\"\"Create a synthetic dataset from a single image using data augmentation\"\"\"\n",
        "        logger.info(f\"Creating synthetic dataset from source image: {source_image_path}\")\n",
        "\n",
        "        if not os.path.isfile(source_image_path):\n",
        "            logger.error(f\"Source image not found: {source_image_path}\")\n",
        "            raise FileNotFoundError(f\"Could not find image file: {source_image_path}\")\n",
        "\n",
        "        img = cv2.imread(source_image_path)\n",
        "        if img is None:\n",
        "            logger.error(f\"Failed to load image: {source_image_path}\")\n",
        "            raise ValueError(f\"OpenCV could not read the image file: {source_image_path}\")\n",
        "\n",
        "        logger.info(f\"Successfully loaded image with shape: {img.shape}\")\n",
        "\n",
        "        tree_samples = self.extract_tree_samples(img)\n",
        "\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.warning(f\"No tree samples extracted. Using fallback approach.\")\n",
        "            tree_samples = self.create_image_patches(img)\n",
        "\n",
        "        logger.info(f\"Created {len(tree_samples)} tree samples\")\n",
        "\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.error(\"Failed to create tree samples\")\n",
        "            raise ValueError(\"Could not create training samples\")\n",
        "\n",
        "        synthetic_data = []\n",
        "        synthetic_labels = []\n",
        "\n",
        "        palm_samples = tree_samples[:len(tree_samples)//2]\n",
        "        coconut_samples = tree_samples[len(tree_samples)//2:]\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(palm_samples)} palm samples\")\n",
        "        for sample in palm_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([0] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(coconut_samples)} coconut samples\")\n",
        "        for sample in coconut_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([1] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(synthetic_data) == 0:\n",
        "            logger.error(\"Failed to create synthetic data\")\n",
        "            raise ValueError(\"Augmentation failed to generate data\")\n",
        "\n",
        "        synthetic_data_array = np.array(synthetic_data)\n",
        "        synthetic_labels_array = np.array(synthetic_labels)\n",
        "\n",
        "        logger.info(f\"Created synthetic dataset with {len(synthetic_data_array)} samples\")\n",
        "        synthetic_data_array = synthetic_data_array / 255.0\n",
        "\n",
        "        return synthetic_data_array, synthetic_labels_array\n",
        "\n",
        "    def create_image_patches(self, image, patch_size=128, stride=64):\n",
        "        \"\"\"Create patches from the image as a fallback method\"\"\"\n",
        "        logger.info(\"Creating image patches as fallback\")\n",
        "        patches = []\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        for y in range(0, h-patch_size, stride):\n",
        "            for x in range(0, w-patch_size, stride):\n",
        "                patch = image[y:y+patch_size, x:x+patch_size]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "                if np.sum(mask) > (patch_size * patch_size * 0.1):\n",
        "                    patch_resized = cv2.resize(patch, (128, 128))\n",
        "                    patches.append(patch_resized)\n",
        "\n",
        "        logger.info(f\"Created {len(patches)} patches from image\")\n",
        "        return patches\n",
        "\n",
        "    def extract_tree_samples(self, image):\n",
        "        \"\"\"Extract individual tree samples using image processing\"\"\"\n",
        "        logger.info(\"Extracting tree samples from image\")\n",
        "\n",
        "        if image is None or image.size == 0:\n",
        "            logger.error(\"Cannot extract tree samples: Input image is empty\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "            kernel = np.ones((5,5), np.uint8)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            min_tree_area = 100\n",
        "            max_tree_area = 5000\n",
        "            tree_samples = []\n",
        "\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if min_tree_area < area < max_tree_area:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    padding = 10\n",
        "                    x_start = max(0, x - padding)\n",
        "                    y_start = max(0, y - padding)\n",
        "                    x_end = min(image.shape[1], x + w + padding)\n",
        "                    y_end = min(image.shape[0], y + h + padding)\n",
        "\n",
        "                    tree_sample = image[y_start:y_end, x_start:x_end]\n",
        "                    if tree_sample.shape[0] > 0 and tree_sample.shape[1] > 0:\n",
        "                        tree_sample = cv2.resize(tree_sample, (128, 128))\n",
        "                        tree_samples.append(tree_sample)\n",
        "\n",
        "            logger.info(f\"Extracted {len(tree_samples)} tree samples\")\n",
        "            return tree_samples\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree sample extraction: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def augment_image(self, image, num_variations=10):\n",
        "        \"\"\"Apply data augmentation to create variations of an image\"\"\"\n",
        "        augmented_images = []\n",
        "\n",
        "        if image is None or image.size == 0:\n",
        "            logger.warning(\"Cannot augment empty image\")\n",
        "            return []\n",
        "\n",
        "        for i in range(num_variations):\n",
        "            try:\n",
        "                img = image.copy()\n",
        "                angle = np.random.uniform(-30, 30)\n",
        "                h, w = img.shape[:2]\n",
        "                M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
        "                img = cv2.warpAffine(img, M, (w, h))\n",
        "\n",
        "                alpha = np.random.uniform(0.8, 1.2)\n",
        "                beta = np.random.uniform(-10, 10)\n",
        "                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = cv2.flip(img, 1)\n",
        "\n",
        "                zoom = np.random.uniform(0.8, 1.2)\n",
        "                h, w = img.shape[:2]\n",
        "                img = cv2.resize(img, None, fx=zoom, fy=zoom)\n",
        "\n",
        "                if zoom > 1:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    start_h = (h_new - h) // 2\n",
        "                    start_w = (w_new - w) // 2\n",
        "                    if start_h < 0 or start_w < 0 or start_h + h > h_new or start_w + w > w_new:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = img[start_h:start_h+h, start_w:start_w+w]\n",
        "                else:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    pad_h = (h - h_new) // 2\n",
        "                    pad_w = (w - w_new) // 2\n",
        "                    if pad_h < 0 or pad_w < 0:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = cv2.copyMakeBorder(img, pad_h, h-h_new-pad_h, pad_w, w-w_new-pad_w, cv2.BORDER_CONSTANT)\n",
        "\n",
        "                img = cv2.resize(img, (128, 128))\n",
        "                augmented_images.append(img)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return augmented_images\n",
        "\n",
        "    def build_tree_classification_model(self):\n",
        "        \"\"\"Build a transfer learning model for tree type classification\"\"\"\n",
        "        logger.info(\"Building tree classification model\")\n",
        "        try:\n",
        "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "            self.tree_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.tree_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            logger.info(\"Tree classification model built successfully\")\n",
        "            return self.tree_model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building tree classification model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def build_health_analysis_model(self):\n",
        "        \"\"\"Build a model for tree health classification\"\"\"\n",
        "        logger.info(\"Building health analysis model\")\n",
        "        try:\n",
        "            base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "            self.health_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.health_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            logger.info(\"Health analysis model built successfully\")\n",
        "            return self.health_model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building health analysis model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train_models(self, source_image_path, manual_labels_path=None):\n",
        "        \"\"\"Train both tree classification and health models\"\"\"\n",
        "        logger.info(f\"Training models using source image: {source_image_path}\")\n",
        "\n",
        "        try:\n",
        "            X, y_tree_type = self.create_synthetic_dataset(source_image_path)\n",
        "            X_train, X_val, y_train_tree, y_val_tree = train_test_split(X, y_tree_type, test_size=0.2, random_state=42)\n",
        "\n",
        "            self.build_tree_classification_model()\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            tree_history = self.tree_model.fit(\n",
        "                X_train, y_train_tree,\n",
        "                validation_data=(X_val, y_val_tree),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Tree model training complete. Final accuracy: {tree_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "            health_features = self.extract_health_features(X)\n",
        "            y_health = self.generate_synthetic_health_labels(health_features)\n",
        "            _, _, y_train_health, y_val_health = train_test_split(X, y_health, test_size=0.2, random_state=42)\n",
        "\n",
        "            self.build_health_analysis_model()\n",
        "            health_history = self.health_model.fit(\n",
        "                X_train, y_train_health,\n",
        "                validation_data=(X_val, y_val_health),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Health model training complete. Final accuracy: {health_history.history['accuracy'][-1]:.4f}\")\n",
        "            return tree_history, health_history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during model training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_health_features(self, images):\n",
        "        \"\"\"Extract features relevant to tree health\"\"\"\n",
        "        logger.info(\"Extracting health features from images\")\n",
        "        health_features = []\n",
        "\n",
        "        for img in images:\n",
        "            try:\n",
        "                if img.dtype != np.uint8:\n",
        "                    img_uint8 = (img * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    img_uint8 = img\n",
        "\n",
        "                hsv = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2HSV)\n",
        "                avg_h = np.mean(hsv[:,:,0])\n",
        "                avg_s = np.mean(hsv[:,:,1])\n",
        "                avg_v = np.mean(hsv[:,:,2])\n",
        "                green_channel = img_uint8[:,:,1]\n",
        "                avg_green = np.mean(green_channel)\n",
        "                gray = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2GRAY)\n",
        "                texture = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                features = [avg_h, avg_s, avg_v, avg_green, texture]\n",
        "                health_features.append(features)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting health features: {str(e)}\")\n",
        "                health_features.append([0, 0, 0, 0, 0])\n",
        "\n",
        "        return np.array(health_features)\n",
        "\n",
        "    def generate_synthetic_health_labels(self, features):\n",
        "        \"\"\"Generate synthetic health labels based on features\"\"\"\n",
        "        logger.info(\"Generating synthetic health labels\")\n",
        "\n",
        "        if features.size == 0:\n",
        "            logger.warning(\"Empty features array\")\n",
        "            return np.array([])\n",
        "\n",
        "        features = np.nan_to_num(features)\n",
        "        features_min = features.min(axis=0)\n",
        "        features_max = features.max(axis=0)\n",
        "        range_values = features_max - features_min\n",
        "        range_values[range_values == 0] = 1\n",
        "        normalized_features = (features - features_min) / range_values\n",
        "\n",
        "        green_idx = 3\n",
        "        value_idx = 2\n",
        "        health_labels = []\n",
        "\n",
        "        for sample in normalized_features:\n",
        "            if sample[green_idx] > 0.75 and sample[value_idx] > 0.6:\n",
        "                label = 3  # Healthy\n",
        "            elif sample[green_idx] > 0.5 and sample[value_idx] > 0.5:\n",
        "                label = 2  # Moderate\n",
        "            elif sample[green_idx] > 0.3 and sample[value_idx] > 0.3:\n",
        "                label = 1  # Declining\n",
        "            else:\n",
        "                label = 0  # Needs inspection\n",
        "            health_labels.append(label)\n",
        "\n",
        "        logger.info(f\"Generated {len(health_labels)} health labels\")\n",
        "        return np.array(health_labels)\n",
        "\n",
        "    def process_and_visualize_tif(self, tif_path, output_jpg_path):\n",
        "        \"\"\"Process an entire TIFF file and generate a JPG output with detections\"\"\"\n",
        "        logger.info(f\"Processing TIFF file: {tif_path}\")\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(tif_path):\n",
        "                logger.error(f\"TIFF file not found: {tif_path}\")\n",
        "                raise FileNotFoundError(f\"TIFF file not found: {tif_path}\")\n",
        "\n",
        "            full_image = imread(tif_path)\n",
        "            if full_image is None:\n",
        "                logger.error(f\"Failed to load TIFF image: {tif_path}\")\n",
        "                raise ValueError(f\"Could not read TIFF image: {tif_path}\")\n",
        "\n",
        "            if len(full_image.shape) == 3 and full_image.shape[2] == 3:\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR)\n",
        "            elif len(full_image.shape) == 2:\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            logger.info(f\"Loaded TIFF image with shape: {full_image.shape}\")\n",
        "            results_df = self.process_drone_image_from_array(full_image)\n",
        "            self.generate_detection_visualization(full_image, results_df, output_jpg_path)\n",
        "\n",
        "            logger.info(f\"Detection complete. Output saved to: {output_jpg_path}\")\n",
        "            return results_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing TIFF file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_drone_image_from_array(self, image_array):\n",
        "        \"\"\"Process a full drone image array and classify all trees\"\"\"\n",
        "        logger.info(f\"Processing image array with shape: {image_array.shape}\")\n",
        "\n",
        "        if image_array is None or image_array.size == 0:\n",
        "            logger.error(\"Invalid image array\")\n",
        "            raise ValueError(\"Invalid image array provided\")\n",
        "\n",
        "        tree_locations, tree_images = self.segment_trees_sliding_window(image_array)\n",
        "\n",
        "        if len(tree_locations) == 0:\n",
        "            logger.warning(\"No trees detected. Using fallback method.\")\n",
        "            tree_locations, tree_images = self.create_grid_samples(image_array)\n",
        "\n",
        "        logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "        results = []\n",
        "        batch_size = 32\n",
        "\n",
        "        for i in range(0, len(tree_locations), batch_size):\n",
        "            batch_locations = tree_locations[i:i+batch_size]\n",
        "            batch_images = tree_images[i:i+batch_size]\n",
        "\n",
        "            preprocessed_batch = []\n",
        "            for img in batch_images:\n",
        "                try:\n",
        "                    img_resized = cv2.resize(img, (128, 128))\n",
        "                    img_normalized = img_resized / 255.0\n",
        "                    preprocessed_batch.append(img_normalized)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error preprocessing image {i}: {str(e)}\")\n",
        "                    preprocessed_batch.append(np.zeros((128, 128, 3)))\n",
        "\n",
        "            input_batch = np.array(preprocessed_batch)\n",
        "\n",
        "            if len(input_batch) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                tree_preds = self.tree_model.predict(input_batch)\n",
        "                health_preds = self.health_model.predict(input_batch)\n",
        "\n",
        "                for j, (location, img, tree_pred, health_pred) in enumerate(\n",
        "                    zip(batch_locations, batch_images, tree_preds, health_preds)\n",
        "                ):\n",
        "                    tree_type = \"Palm\" if np.argmax(tree_pred) == 0 else \"Coconut\"\n",
        "                    tree_confidence = float(np.max(tree_pred))\n",
        "                    health_idx = np.argmax(health_pred)\n",
        "                    health_categories = [\"Needs Inspection\", \"Declining Health\", \"Moderate\", \"Healthy\"]\n",
        "                    health_status = health_categories[health_idx]\n",
        "                    health_confidence = float(np.max(health_pred))\n",
        "                    x, y = location\n",
        "                    results.append({\n",
        "                        \"Tree_ID\": i+j+1,\n",
        "                        \"Type\": tree_type,\n",
        "                        \"Type_Confidence\": tree_confidence,\n",
        "                        \"Health\": health_status,\n",
        "                        \"Health_Confidence\": health_confidence,\n",
        "                        \"X_coordinate\": int(x),\n",
        "                        \"Y_coordinate\": int(y)\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during batch prediction: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        results_df = pd.DataFrame(results) if results else pd.DataFrame([{\n",
        "            \"Tree_ID\": 1, \"Type\": \"Palm\", \"Type_Confidence\": 0.8,\n",
        "            \"Health\": \"Healthy\", \"Health_Confidence\": 0.7,\n",
        "            \"X_coordinate\": 100, \"Y_coordinate\": 100\n",
        "        }])\n",
        "\n",
        "        logger.info(f\"Created results dataframe with {len(results_df)} trees\")\n",
        "        return results_df\n",
        "\n",
        "    def generate_detection_visualization(self, image, results_df, output_path):\n",
        "        \"\"\"Generate and save a visualization of detection results as JPG\"\"\"\n",
        "        logger.info(f\"Generating visualization for output: {output_path}\")\n",
        "\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(16, 12))\n",
        "            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            ax.imshow(img_rgb)\n",
        "\n",
        "            health_colors = {\n",
        "                \"Healthy\": \"green\",\n",
        "                \"Moderate\": \"yellowgreen\",\n",
        "                \"Declining Health\": \"orange\",\n",
        "                \"Needs Inspection\": \"red\"\n",
        "            }\n",
        "\n",
        "            for _, row in results_df.iterrows():\n",
        "                x, y = row[\"X_coordinate\"], row[\"Y_coordinate\"]\n",
        "                tree_type = row[\"Type\"]\n",
        "                health_status = row[\"Health\"]\n",
        "\n",
        "                circle = plt.Circle((x, y), 20, color=health_colors[health_status],\n",
        "                                  fill=False, linewidth=2)\n",
        "                ax.add_patch(circle)\n",
        "\n",
        "                label = f\"{tree_type[0]}\\n{health_status[0]}\"\n",
        "                ax.text(x, y, label, color='white', fontsize=8,\n",
        "                       ha='center', va='center', bbox=dict(facecolor=health_colors[health_status],\n",
        "                                                         alpha=0.5))\n",
        "\n",
        "            ax.set_title(\"Tree Detection Results\")\n",
        "            ax.axis('off')\n",
        "            plt.savefig(output_path, format='jpg', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Visualization saved to {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating visualization: {str(e)}\")\n",
        "            blank_img = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
        "            cv2.imwrite(output_path, blank_img)\n",
        "\n",
        "    def create_grid_samples(self, image, grid_size=50):\n",
        "        \"\"\"Create a grid of sample points as a fallback method\"\"\"\n",
        "        logger.info(\"Creating grid samples as fallback\")\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "\n",
        "        for y in range(grid_size, h-grid_size, grid_size):\n",
        "            for x in range(grid_size, w-grid_size, grid_size):\n",
        "                patch = image[y-grid_size//2:y+grid_size//2, x-grid_size//2:x+grid_size//2]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "                if np.sum(mask) > (grid_size * grid_size * 0.1):\n",
        "                    tree_locations.append((x, y))\n",
        "                    tree_images.append(patch)\n",
        "\n",
        "        logger.info(f\"Created {len(tree_locations)} grid samples\")\n",
        "        return tree_locations, tree_images\n",
        "\n",
        "    def segment_trees_sliding_window(self, image, window_size=128, stride=64):\n",
        "        \"\"\"Segment trees using a sliding window approach\"\"\"\n",
        "        logger.info(\"Segmenting trees using sliding window approach\")\n",
        "\n",
        "        if image is None or image.shape[0] == 0 or image.shape[1] == 0:\n",
        "            logger.error(\"Cannot segment trees: Invalid image\")\n",
        "            return [], []\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            veg_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "            for y in range(0, h-window_size, stride):\n",
        "                for x in range(0, w-window_size, stride):\n",
        "                    window = image[y:y+window_size, x:x+window_size]\n",
        "                    mask_window = veg_mask[y:y+window_size, x:x+window_size]\n",
        "\n",
        "                    veg_ratio = np.sum(mask_window > 0) / (window_size * window_size)\n",
        "\n",
        "                    if veg_ratio > 0.3:\n",
        "                        mask_copy = mask_window.copy()\n",
        "                        contours, _ = cv2.findContours(mask_copy, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                        for contour in contours:\n",
        "                            area = cv2.contourArea(contour)\n",
        "                            if area > 100:\n",
        "                                perimeter = cv2.arcLength(contour, True)\n",
        "                                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
        "\n",
        "                                if circularity > 0.4:\n",
        "                                    M = cv2.moments(contour)\n",
        "                                    if M[\"m00\"] > 0:\n",
        "                                        c_x = int(M[\"m10\"] / M[\"m00\"]) + x\n",
        "                                        c_y = int(M[\"m01\"] / M[\"m00\"]) + y\n",
        "\n",
        "                                        tree_x1 = max(0, c_x - window_size//2)\n",
        "                                        tree_y1 = max(0, c_y - window_size//2)\n",
        "                                        tree_x2 = min(w, c_x + window_size//2)\n",
        "                                        tree_y2 = min(h, c_y + window_size//2)\n",
        "\n",
        "                                        tree_img = image[tree_y1:tree_y2, tree_x1:tree_x2]\n",
        "                                        if tree_img.shape[0] > 0 and tree_img.shape[1] > 0:\n",
        "                                            tree_img_resized = cv2.resize(tree_img, (128, 128))\n",
        "                                            tree_images.append(tree_img_resized)\n",
        "                                            tree_locations.append((c_x, c_y))\n",
        "\n",
        "            logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "            return tree_locations, tree_images\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree segmentation: {str(e)}\")\n",
        "            return [], []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = PalmTreeAnalysis()\n",
        "\n",
        "    # Training\n",
        "    source_image = \"/content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    analyzer.train_models(source_image)\n",
        "\n",
        "    # Testing with TIFF file\n",
        "    tif_file = \"/content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    output_jpg = \"path/to/output_detections.jpg\"\n",
        "    results = analyzer.process_and_visualize_tif(tif_file, output_jpg)\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "jF_Bu1lxEPcX",
        "outputId": "6d2cc072-3833-49bb-b4c9-2db6f70e5a8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Failed to load image: /content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\n",
            "ERROR:__main__:Error during model training: OpenCV could not read the image file: /content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "OpenCV could not read the image file: /content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-970024782a16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0msource_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;31m# Testing with TIFF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-970024782a16>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, source_image_path, manual_labels_path)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tree_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_synthetic_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tree_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-970024782a16>\u001b[0m in \u001b[0;36mcreate_synthetic_dataset\u001b[0;34m(self, source_image_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load image: {source_image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OpenCV could not read the image file: {source_image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Successfully loaded image with shape: {img.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: OpenCV could not read the image file: /content/4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tifffile import imread  # Added for TIFF file handling\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler('palm_tree_analysis.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PalmTreeAnalysis:\n",
        "    def __init__(self):\n",
        "        self.tree_model = None\n",
        "        self.health_model = None\n",
        "        logger.info(\"PalmTreeAnalysis initialized\")\n",
        "\n",
        "    def create_synthetic_dataset(self, source_image_path):\n",
        "        \"\"\"Create a synthetic dataset from a single image using data augmentation\"\"\"\n",
        "        logger.info(f\"Creating synthetic dataset from source image: {source_image_path}\")\n",
        "        if not os.path.isfile(source_image_path):\n",
        "            logger.error(f\"Source image not found: {source_image_path}\")\n",
        "            raise FileNotFoundError(f\"Could not find image file: {source_image_path}\")\n",
        "\n",
        "        # Load the TIFF file using tifffile\n",
        "        img = imread(source_image_path)\n",
        "        if img is None:\n",
        "            logger.error(f\"Failed to load image: {source_image_path}\")\n",
        "            raise ValueError(f\"Could not read the image file: {source_image_path}\")\n",
        "\n",
        "        # Convert to a compatible format (e.g., 8-bit RGB or grayscale)\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3:  # RGB image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV compatibility\n",
        "        elif len(img.shape) == 2:  # Grayscale image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # Convert to 3-channel grayscale\n",
        "        else:\n",
        "            logger.error(\"Unsupported image format. Expected 2D grayscale or 3D RGB image.\")\n",
        "            raise ValueError(\"Unsupported image format.\")\n",
        "\n",
        "        logger.info(f\"Successfully loaded image with shape: {img.shape}\")\n",
        "\n",
        "        # Proceed with tree sample extraction and augmentation\n",
        "        tree_samples = self.extract_tree_samples(img)\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.warning(f\"No tree samples extracted. Using fallback approach.\")\n",
        "            tree_samples = self.create_image_patches(img)\n",
        "        logger.info(f\"Created {len(tree_samples)} tree samples\")\n",
        "\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.error(\"Failed to create tree samples\")\n",
        "            raise ValueError(\"Could not create training samples\")\n",
        "\n",
        "        synthetic_data = []\n",
        "        synthetic_labels = []\n",
        "        palm_samples = tree_samples[:len(tree_samples)//2]\n",
        "        coconut_samples = tree_samples[len(tree_samples)//2:]\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(palm_samples)} palm samples\")\n",
        "        for sample in palm_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([0] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(coconut_samples)} coconut samples\")\n",
        "        for sample in coconut_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([1] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(synthetic_data) == 0:\n",
        "            logger.error(\"Failed to create synthetic data\")\n",
        "            raise ValueError(\"Augmentation failed to generate data\")\n",
        "\n",
        "        synthetic_data_array = np.array(synthetic_data)\n",
        "        synthetic_labels_array = np.array(synthetic_labels)\n",
        "        logger.info(f\"Created synthetic dataset with {len(synthetic_data_array)} samples\")\n",
        "        synthetic_data_array = synthetic_data_array / 255.0\n",
        "        return synthetic_data_array, synthetic_labels_array\n",
        "\n",
        "    def create_image_patches(self, image, patch_size=128, stride=64):\n",
        "        \"\"\"Create patches from the image as a fallback method\"\"\"\n",
        "        logger.info(\"Creating image patches as fallback\")\n",
        "        patches = []\n",
        "        h, w = image.shape[:2]\n",
        "        for y in range(0, h-patch_size, stride):\n",
        "            for x in range(0, w-patch_size, stride):\n",
        "                patch = image[y:y+patch_size, x:x+patch_size]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "                if np.sum(mask) > (patch_size * patch_size * 0.1):\n",
        "                    patch_resized = cv2.resize(patch, (128, 128))\n",
        "                    patches.append(patch_resized)\n",
        "        logger.info(f\"Created {len(patches)} patches from image\")\n",
        "        return patches\n",
        "\n",
        "    def extract_tree_samples(self, image):\n",
        "        \"\"\"Extract individual tree samples using image processing\"\"\"\n",
        "        logger.info(\"Extracting tree samples from image\")\n",
        "        if image is None or image.size == 0:\n",
        "            logger.error(\"Cannot extract tree samples: Input image is empty\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "            kernel = np.ones((5,5), np.uint8)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            min_tree_area = 100\n",
        "            max_tree_area = 5000\n",
        "            tree_samples = []\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if min_tree_area < area < max_tree_area:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    padding = 10\n",
        "                    x_start = max(0, x - padding)\n",
        "                    y_start = max(0, y - padding)\n",
        "                    x_end = min(image.shape[1], x + w + padding)\n",
        "                    y_end = min(image.shape[0], y + h + padding)\n",
        "                    tree_sample = image[y_start:y_end, x_start:x_end]\n",
        "                    if tree_sample.shape[0] > 0 and tree_sample.shape[1] > 0:\n",
        "                        tree_sample = cv2.resize(tree_sample, (128, 128))\n",
        "                        tree_samples.append(tree_sample)\n",
        "\n",
        "            logger.info(f\"Extracted {len(tree_samples)} tree samples\")\n",
        "            return tree_samples\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree sample extraction: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def augment_image(self, image, num_variations=10):\n",
        "        \"\"\"Apply data augmentation to create variations of an image\"\"\"\n",
        "        augmented_images = []\n",
        "        if image is None or image.size == 0:\n",
        "            logger.warning(\"Cannot augment empty image\")\n",
        "            return []\n",
        "\n",
        "        for i in range(num_variations):\n",
        "            try:\n",
        "                img = image.copy()\n",
        "                angle = np.random.uniform(-30, 30)\n",
        "                h, w = img.shape[:2]\n",
        "                M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
        "                img = cv2.warpAffine(img, M, (w, h))\n",
        "\n",
        "                alpha = np.random.uniform(0.8, 1.2)\n",
        "                beta = np.random.uniform(-10, 10)\n",
        "                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = cv2.flip(img, 1)\n",
        "\n",
        "                zoom = np.random.uniform(0.8, 1.2)\n",
        "                h, w = img.shape[:2]\n",
        "                img = cv2.resize(img, None, fx=zoom, fy=zoom)\n",
        "\n",
        "                if zoom > 1:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    start_h = (h_new - h) // 2\n",
        "                    start_w = (w_new - w) // 2\n",
        "                    if start_h < 0 or start_w < 0 or start_h + h > h_new or start_w + w > w_new:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = img[start_h:start_h+h, start_w:start_w+w]\n",
        "                else:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    pad_h = (h - h_new) // 2\n",
        "                    pad_w = (w - w_new) // 2\n",
        "                    if pad_h < 0 or pad_w < 0:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = cv2.copyMakeBorder(img, pad_h, h-h_new-pad_h, pad_w, w-w_new-pad_w, cv2.BORDER_CONSTANT)\n",
        "\n",
        "                img = cv2.resize(img, (128, 128))\n",
        "                augmented_images.append(img)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {str(e)}\")\n",
        "                continue\n",
        "        return augmented_images\n",
        "\n",
        "    def build_tree_classification_model(self):\n",
        "        \"\"\"Build a transfer learning model for tree type classification\"\"\"\n",
        "        logger.info(\"Building tree classification model\")\n",
        "        try:\n",
        "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(2, activation='softmax')(x)\n",
        "            self.tree_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.tree_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            logger.info(\"Tree classification model built successfully\")\n",
        "            return self.tree_model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building tree classification model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def build_health_analysis_model(self):\n",
        "        \"\"\"Build a model for tree health classification\"\"\"\n",
        "        logger.info(\"Building health analysis model\")\n",
        "        try:\n",
        "            base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(4, activation='softmax')(x)\n",
        "            self.health_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.health_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            logger.info(\"Health analysis model built successfully\")\n",
        "            return self.health_model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building health analysis model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train_models(self, source_image_path, manual_labels_path=None):\n",
        "        \"\"\"Train both tree classification and health models\"\"\"\n",
        "        logger.info(f\"Training models using source image: {source_image_path}\")\n",
        "        try:\n",
        "            X, y_tree_type = self.create_synthetic_dataset(source_image_path)\n",
        "            X_train, X_val, y_train_tree, y_val_tree = train_test_split(X, y_tree_type, test_size=0.2, random_state=42)\n",
        "            self.build_tree_classification_model()\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "            tree_history = self.tree_model.fit(\n",
        "                X_train, y_train_tree,\n",
        "                validation_data=(X_val, y_val_tree),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "            logger.info(f\"Tree model training complete. Final accuracy: {tree_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "            health_features = self.extract_health_features(X)\n",
        "            y_health = self.generate_synthetic_health_labels(health_features)\n",
        "            _, _, y_train_health, y_val_health = train_test_split(X, y_health, test_size=0.2, random_state=42)\n",
        "            self.build_health_analysis_model()\n",
        "            health_history = self.health_model.fit(\n",
        "                X_train, y_train_health,\n",
        "                validation_data=(X_val, y_val_health),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "            logger.info(f\"Health model training complete. Final accuracy: {health_history.history['accuracy'][-1]:.4f}\")\n",
        "            return tree_history, health_history\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during model training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_health_features(self, images):\n",
        "        \"\"\"Extract features relevant to tree health\"\"\"\n",
        "        logger.info(\"Extracting health features from images\")\n",
        "        health_features = []\n",
        "        for img in images:\n",
        "            try:\n",
        "                if img.dtype != np.uint8:\n",
        "                    img_uint8 = (img * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    img_uint8 = img\n",
        "                hsv = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2HSV)\n",
        "                avg_h = np.mean(hsv[:,:,0])\n",
        "                avg_s = np.mean(hsv[:,:,1])\n",
        "                avg_v = np.mean(hsv[:,:,2])\n",
        "                green_channel = img_uint8[:,:,1]\n",
        "                avg_green = np.mean(green_channel)\n",
        "                gray = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2GRAY)\n",
        "                texture = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                features = [avg_h, avg_s, avg_v, avg_green, texture]\n",
        "                health_features.append(features)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting health features: {str(e)}\")\n",
        "                health_features.append([0, 0, 0, 0, 0])\n",
        "        return np.array(health_features)\n",
        "\n",
        "    def generate_synthetic_health_labels(self, features):\n",
        "        \"\"\"Generate synthetic health labels based on features\"\"\"\n",
        "        logger.info(\"Generating synthetic health labels\")\n",
        "        if features.size == 0:\n",
        "            logger.warning(\"Empty features array\")\n",
        "            return np.array([])\n",
        "\n",
        "        features = np.nan_to_num(features)\n",
        "        features_min = features.min(axis=0)\n",
        "        features_max = features.max(axis=0)\n",
        "        range_values = features_max - features_min\n",
        "        range_values[range_values == 0] = 1\n",
        "        normalized_features = (features - features_min) / range_values\n",
        "\n",
        "        green_idx = 3\n",
        "        value_idx = 2\n",
        "        health_labels = []\n",
        "        for sample in normalized_features:\n",
        "            if sample[green_idx] > 0.75 and sample[value_idx] > 0.6:\n",
        "                label = 3  # Healthy\n",
        "            elif sample[green_idx] > 0.5 and sample[value_idx] > 0.5:\n",
        "                label = 2  # Moderate\n",
        "            elif sample[green_idx] > 0.3 and sample[value_idx] > 0.3:\n",
        "                label = 1  # Declining\n",
        "            else:\n",
        "                label = 0  # Needs inspection\n",
        "            health_labels.append(label)\n",
        "\n",
        "        logger.info(f\"Generated {len(health_labels)} health labels\")\n",
        "        return np.array(health_labels)\n",
        "\n",
        "    def process_and_visualize_tif(self, tif_path, output_jpg_path):\n",
        "        \"\"\"Process an entire TIFF file and generate a JPG output with detections\"\"\"\n",
        "        logger.info(f\"Processing TIFF file: {tif_path}\")\n",
        "        try:\n",
        "            if not os.path.exists(tif_path):\n",
        "                logger.error(f\"TIFF file not found: {tif_path}\")\n",
        "                raise FileNotFoundError(f\"TIFF file not found: {tif_path}\")\n",
        "\n",
        "            full_image = imread(tif_path)\n",
        "            if full_image is None:\n",
        "                logger.error(f\"Failed to load TIFF image: {tif_path}\")\n",
        "                raise ValueError(f\"Could not read TIFF image: {tif_path}\")\n",
        "\n",
        "            # Convert to a compatible format (e.g., 8-bit RGB or grayscale)\n",
        "            if len(full_image.shape) == 3 and full_image.shape[2] == 3:  # RGB image\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR)\n",
        "            elif len(full_image.shape) == 2:  # Grayscale image\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            logger.info(f\"Loaded TIFF image with shape: {full_image.shape}\")\n",
        "            results_df = self.process_drone_image_from_array(full_image)\n",
        "            self.generate_detection_visualization(full_image, results_df, output_jpg_path)\n",
        "            logger.info(f\"Detection complete. Output saved to: {output_jpg_path}\")\n",
        "            return results_df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing TIFF file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_drone_image_from_array(self, image_array):\n",
        "        \"\"\"Process a full drone image array and classify all trees\"\"\"\n",
        "        logger.info(f\"Processing image array with shape: {image_array.shape}\")\n",
        "        if image_array is None or image_array.size == 0:\n",
        "            logger.error(\"Invalid image array\")\n",
        "            raise ValueError(\"Invalid image array provided\")\n",
        "\n",
        "        tree_locations, tree_images = self.segment_trees_sliding_window(image_array)\n",
        "        if len(tree_locations) == 0:\n",
        "            logger.warning(\"No trees detected. Using fallback method.\")\n",
        "            tree_locations, tree_images = self.create_grid_samples(image_array)\n",
        "\n",
        "        logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "        results = []\n",
        "        batch_size = 32\n",
        "\n",
        "        for i in range(0, len(tree_locations), batch_size):\n",
        "            batch_locations = tree_locations[i:i+batch_size]\n",
        "            batch_images = tree_images[i:i+batch_size]\n",
        "            preprocessed_batch = []\n",
        "\n",
        "            for img in batch_images:\n",
        "                try:\n",
        "                    img_resized = cv2.resize(img, (128, 128))\n",
        "                    img_normalized = img_resized / 255.0\n",
        "                    preprocessed_batch.append(img_normalized)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error preprocessing image {i}: {str(e)}\")\n",
        "                    preprocessed_batch.append(np.zeros((128, 128, 3)))\n",
        "\n",
        "            input_batch = np.array(preprocessed_batch)\n",
        "            if len(input_batch) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                tree_preds = self.tree_model.predict(input_batch)\n",
        "                health_preds = self.health_model.predict(input_batch)\n",
        "                for j, (location, img, tree_pred, health_pred) in enumerate(\n",
        "                    zip(batch_locations, batch_images, tree_preds, health_preds)\n",
        "                ):\n",
        "                    tree_type = \"Palm\" if np.argmax(tree_pred) == 0 else \"Coconut\"\n",
        "                    tree_confidence = float(np.max(tree_pred))\n",
        "                    health_idx = np.argmax(health_pred)\n",
        "                    health_categories = [\"Needs Inspection\", \"Declining Health\", \"Moderate\", \"Healthy\"]\n",
        "                    health_status = health_categories[health_idx]\n",
        "                    health_confidence = float(np.max(health_pred))\n",
        "                    x, y = location\n",
        "                    results.append({\n",
        "                        \"Tree_ID\": i+j+1,\n",
        "                        \"Type\": tree_type,\n",
        "                        \"Type_Confidence\": tree_confidence,\n",
        "                        \"Health\": health_status,\n",
        "                        \"Health_Confidence\": health_confidence,\n",
        "                        \"X_coordinate\": int(x),\n",
        "                        \"Y_coordinate\": int(y)\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during batch prediction: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        results_df = pd.DataFrame(results) if results else pd.DataFrame([{\n",
        "            \"Tree_ID\": 1, \"Type\": \"Palm\", \"Type_Confidence\": 0.8,\n",
        "            \"Health\": \"Healthy\", \"Health_Confidence\": 0.7,\n",
        "            \"X_coordinate\": 100, \"Y_coordinate\": 100\n",
        "        }])\n",
        "        logger.info(f\"Created results dataframe with {len(results_df)} trees\")\n",
        "        return results_df\n",
        "\n",
        "    def generate_detection_visualization(self, image, results_df, output_path):\n",
        "        \"\"\"Generate and save a visualization of detection results as JPG\"\"\"\n",
        "        logger.info(f\"Generating visualization for output: {output_path}\")\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(16, 12))\n",
        "            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            ax.imshow(img_rgb)\n",
        "            health_colors = {\n",
        "                \"Healthy\": \"green\",\n",
        "                \"Moderate\": \"yellowgreen\",\n",
        "                \"Declining Health\": \"orange\",\n",
        "                \"Needs Inspection\": \"red\"\n",
        "            }\n",
        "\n",
        "            for _, row in results_df.iterrows():\n",
        "                x, y = row[\"X_coordinate\"], row[\"Y_coordinate\"]\n",
        "                tree_type = row[\"Type\"]\n",
        "                health_status = row[\"Health\"]\n",
        "                circle = plt.Circle((x, y), 20, color=health_colors[health_status],\n",
        "                                  fill=False, linewidth=2)\n",
        "                ax.add_patch(circle)\n",
        "                label = f\"{tree_type[0]}\\n{health_status[0]}\"\n",
        "                ax.text(x, y, label, color='white', fontsize=8,\n",
        "                       ha='center', va='center', bbox=dict(facecolor=health_colors[health_status],\n",
        "                                                         alpha=0.5))\n",
        "\n",
        "            ax.set_title(\"Tree Detection Results\")\n",
        "            ax.axis('off')\n",
        "            plt.savefig(output_path, format='jpg', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            logger.info(f\"Visualization saved to {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating visualization: {str(e)}\")\n",
        "            blank_img = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
        "            cv2.imwrite(output_path, blank_img)\n",
        "\n",
        "    def create_grid_samples(self, image, grid_size=50):\n",
        "        \"\"\"Create a grid of sample points as a fallback method\"\"\"\n",
        "        logger.info(\"Creating grid samples as fallback\")\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "\n",
        "        for y in range(grid_size, h-grid_size, grid_size):\n",
        "            for x in range(grid_size, w-grid_size, grid_size):\n",
        "                patch = image[y-grid_size//2:y+grid_size//2, x-grid_size//2:x+grid_size//2]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "                if np.sum(mask) > (grid_size * grid_size * 0.1):\n",
        "                    tree_locations.append((x, y))\n",
        "                    tree_images.append(patch)\n",
        "\n",
        "        logger.info(f\"Created {len(tree_locations)} grid samples\")\n",
        "        return tree_locations, tree_images\n",
        "\n",
        "    def segment_trees_sliding_window(self, image, window_size=128, stride=64):\n",
        "        \"\"\"Segment trees using a sliding window approach\"\"\"\n",
        "        logger.info(\"Segmenting trees using sliding window approach\")\n",
        "        if image is None or image.shape[0] == 0 or image.shape[1] == 0:\n",
        "            logger.error(\"Cannot segment trees: Invalid image\")\n",
        "            return [], []\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            veg_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "            for y in range(0, h-window_size, stride):\n",
        "                for x in range(0, w-window_size, stride):\n",
        "                    window = image[y:y+window_size, x:x+window_size]\n",
        "                    mask_window = veg_mask[y:y+window_size, x:x+window_size]\n",
        "                    veg_ratio = np.sum(mask_window > 0) / (window_size * window_size)\n",
        "\n",
        "                    if veg_ratio > 0.3:\n",
        "                        mask_copy = mask_window.copy()\n",
        "                        contours, _ = cv2.findContours(mask_copy, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                        for contour in contours:\n",
        "                            area = cv2.contourArea(contour)\n",
        "                            if area > 100:\n",
        "                                perimeter = cv2.arcLength(contour, True)\n",
        "                                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
        "\n",
        "                                if circularity > 0.4:\n",
        "                                    M = cv2.moments(contour)\n",
        "                                    if M[\"m00\"] > 0:\n",
        "                                        c_x = int(M[\"m10\"] / M[\"m00\"]) + x\n",
        "                                        c_y = int(M[\"m01\"] / M[\"m00\"]) + y\n",
        "                                        tree_x1 = max(0, c_x - window_size//2)\n",
        "                                        tree_y1 = max(0, c_y - window_size//2)\n",
        "                                        tree_x2 = min(w, c_x + window_size//2)\n",
        "                                        tree_y2 = min(h, c_y + window_size//2)\n",
        "                                        tree_img = image[tree_y1:tree_y2, tree_x1:tree_x2]\n",
        "\n",
        "                                        if tree_img.shape[0] > 0 and tree_img.shape[1] > 0:\n",
        "                                            tree_img_resized = cv2.resize(tree_img, (128, 128))\n",
        "                                            tree_images.append(tree_img_resized)\n",
        "                                            tree_locations.append((c_x, c_y))\n",
        "\n",
        "            logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "            return tree_locations, tree_images\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree segmentation: {str(e)}\")\n",
        "            return [], []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = PalmTreeAnalysis()\n",
        "\n",
        "    # Training\n",
        "    source_image = \"4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    analyzer.train_models(source_image)\n",
        "\n",
        "    # Testing with TIFF file\n",
        "    tif_file = \"4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    output_jpg = \"path/to/output_detections.jpg\"\n",
        "    results = analyzer.process_and_visualize_tif(tif_file, output_jpg)\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "RsIf6a6MEi-w",
        "outputId": "761f93ad-aa31-4e4c-ea91-7670cc95661e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error during model training: <COMPRESSION.JPEG: 7> requires the 'imagecodecs' package\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "<COMPRESSION.JPEG: 7> requires the 'imagecodecs' package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e1437d80e9a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0msource_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;31m# Testing with TIFF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e1437d80e9a1>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, source_image_path, manual_labels_path)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training models using source image: {source_image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tree_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_synthetic_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tree_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e1437d80e9a1>\u001b[0m in \u001b[0;36mcreate_synthetic_dataset\u001b[0;34m(self, source_image_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Load the TIFF file using tifffile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load image: {source_image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(files, selection, aszarr, key, series, level, squeeze, maxworkers, buffersize, mode, name, offset, size, pattern, axesorder, categories, imread, imreadargs, sort, container, chunkshape, chunkdtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mzarr_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                 return tif.asarray(\n\u001b[0m\u001b[1;32m   1250\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                     \u001b[0mseries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   4507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpage0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page is None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             result = page0.asarray(\n\u001b[0m\u001b[1;32m   4510\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, out, squeeze, lock, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   8845\u001b[0m                 \u001b[0;31m#     pass  # corrupted file, for example, with too many strips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8847\u001b[0;31m             for _ in self.segments(\n\u001b[0m\u001b[1;32m   8848\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8849\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36msegments\u001b[0;34m(self, lock, maxworkers, func, sort, buffersize, _fullsize)\u001b[0m\n\u001b[1;32m   8645\u001b[0m                 \u001b[0mflat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8646\u001b[0m             ):\n\u001b[0;32m-> 8647\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8649\u001b[0m             \u001b[0;31m# reduce memory overhead by processing chunks of up to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(args, decodeargs, decode)\u001b[0m\n\u001b[1;32m   8632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8633\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodeargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8634\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxworkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmaxworkers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mdecode_raise_compression\u001b[0;34m(exc, *args, **kwargs)\u001b[0m\n\u001b[1;32m   8056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8057\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdecode_raise_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8058\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{exc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_raise_compression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: <COMPRESSION.JPEG: 7> requires the 'imagecodecs' package"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKGeju0qGVUw",
        "outputId": "469b1cc5-79e1-4103-e0af-802724d8228f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagecodecs) (1.26.4)\n",
            "Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagecodecs\n",
            "Successfully installed imagecodecs-2024.12.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tifffile import imread  # Added for TIFF file handling\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler('palm_tree_analysis.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PalmTreeAnalysis:\n",
        "    def __init__(self):\n",
        "        self.tree_model = None\n",
        "        self.health_model = None\n",
        "        logger.info(\"PalmTreeAnalysis initialized\")\n",
        "\n",
        "    def create_synthetic_dataset(self, source_image_path):\n",
        "        \"\"\"Create a synthetic dataset from a single image using data augmentation\"\"\"\n",
        "        logger.info(f\"Creating synthetic dataset from source image: {source_image_path}\")\n",
        "        if not os.path.isfile(source_image_path):\n",
        "            logger.error(f\"Source image not found: {source_image_path}\")\n",
        "            raise FileNotFoundError(f\"Could not find image file: {source_image_path}\")\n",
        "\n",
        "        # Attempt to load the TIFF file using tifffile\n",
        "        try:\n",
        "            img = imread(source_image_path)\n",
        "            logger.info(f\"Successfully loaded TIFF image using tifffile\")\n",
        "        except ValueError as e:\n",
        "            logger.warning(f\"Failed to load TIFF using tifffile: {str(e)}. Falling back to OpenCV.\")\n",
        "            img = cv2.imread(source_image_path, cv2.IMREAD_UNCHANGED)\n",
        "            if img is None:\n",
        "                logger.error(f\"Failed to load image using OpenCV: {source_image_path}\")\n",
        "                raise ValueError(f\"Could not read image file: {source_image_path}\")\n",
        "\n",
        "        # Convert to a compatible format (e.g., 8-bit RGB or grayscale)\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3:  # RGB image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        elif len(img.shape) == 2:  # Grayscale image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "        logger.info(f\"Successfully loaded image with shape: {img.shape}\")\n",
        "\n",
        "        tree_samples = self.extract_tree_samples(img)\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.warning(f\"No tree samples extracted. Using fallback approach.\")\n",
        "            tree_samples = self.create_image_patches(img)\n",
        "        logger.info(f\"Created {len(tree_samples)} tree samples\")\n",
        "\n",
        "        if len(tree_samples) == 0:\n",
        "            logger.error(\"Failed to create tree samples\")\n",
        "            raise ValueError(\"Could not create training samples\")\n",
        "\n",
        "        synthetic_data = []\n",
        "        synthetic_labels = []\n",
        "        palm_samples = tree_samples[:len(tree_samples)//2]\n",
        "        coconut_samples = tree_samples[len(tree_samples)//2:]\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(palm_samples)} palm samples\")\n",
        "        for sample in palm_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([0] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"Creating augmented data for {len(coconut_samples)} coconut samples\")\n",
        "        for sample in coconut_samples:\n",
        "            try:\n",
        "                augmented = self.augment_image(sample, num_variations=10)\n",
        "                synthetic_data.extend(augmented)\n",
        "                synthetic_labels.extend([1] * len(augmented))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(synthetic_data) == 0:\n",
        "            logger.error(\"Failed to create synthetic data\")\n",
        "            raise ValueError(\"Augmentation failed to generate data\")\n",
        "\n",
        "        synthetic_data_array = np.array(synthetic_data)\n",
        "        synthetic_labels_array = np.array(synthetic_labels)\n",
        "        logger.info(f\"Created synthetic dataset with {len(synthetic_data_array)} samples\")\n",
        "        synthetic_data_array = synthetic_data_array / 255.0\n",
        "        return synthetic_data_array, synthetic_labels_array\n",
        "\n",
        "    def create_image_patches(self, image, patch_size=128, stride=64):\n",
        "        \"\"\"Create patches from the image as a fallback method\"\"\"\n",
        "        logger.info(\"Creating image patches as fallback\")\n",
        "        patches = []\n",
        "        h, w = image.shape[:2]\n",
        "        for y in range(0, h-patch_size, stride):\n",
        "            for x in range(0, w-patch_size, stride):\n",
        "                patch = image[y:y+patch_size, x:x+patch_size]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "                if np.sum(mask) > (patch_size * patch_size * 0.1):\n",
        "                    patch_resized = cv2.resize(patch, (128, 128))\n",
        "                    patches.append(patch_resized)\n",
        "        logger.info(f\"Created {len(patches)} patches from image\")\n",
        "        return patches\n",
        "\n",
        "    def extract_tree_samples(self, image):\n",
        "        \"\"\"Extract individual tree samples using image processing\"\"\"\n",
        "        logger.info(\"Extracting tree samples from image\")\n",
        "        if image is None or image.size == 0:\n",
        "            logger.error(\"Cannot extract tree samples: Input image is empty\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "            kernel = np.ones((5, 5), np.uint8)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            min_tree_area = 100\n",
        "            max_tree_area = 5000\n",
        "            tree_samples = []\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if min_tree_area < area < max_tree_area:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    padding = 10\n",
        "                    x_start = max(0, x - padding)\n",
        "                    y_start = max(0, y - padding)\n",
        "                    x_end = min(image.shape[1], x + w + padding)\n",
        "                    y_end = min(image.shape[0], y + h + padding)\n",
        "                    tree_sample = image[y_start:y_end, x_start:x_end]\n",
        "                    if tree_sample.shape[0] > 0 and tree_sample.shape[1] > 0:\n",
        "                        tree_sample = cv2.resize(tree_sample, (128, 128))\n",
        "                        tree_samples.append(tree_sample)\n",
        "\n",
        "            logger.info(f\"Extracted {len(tree_samples)} tree samples\")\n",
        "            return tree_samples\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree sample extraction: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def augment_image(self, image, num_variations=10):\n",
        "        \"\"\"Apply data augmentation to create variations of an image\"\"\"\n",
        "        augmented_images = []\n",
        "        if image is None or image.size == 0:\n",
        "            logger.warning(\"Cannot augment empty image\")\n",
        "            return []\n",
        "\n",
        "        for i in range(num_variations):\n",
        "            try:\n",
        "                img = image.copy()\n",
        "                angle = np.random.uniform(-30, 30)\n",
        "                h, w = img.shape[:2]\n",
        "                M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
        "                img = cv2.warpAffine(img, M, (w, h))\n",
        "\n",
        "                alpha = np.random.uniform(0.8, 1.2)\n",
        "                beta = np.random.uniform(-10, 10)\n",
        "                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = cv2.flip(img, 1)\n",
        "\n",
        "                zoom = np.random.uniform(0.8, 1.2)\n",
        "                h, w = img.shape[:2]\n",
        "                img = cv2.resize(img, None, fx=zoom, fy=zoom)\n",
        "\n",
        "                if zoom > 1:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    start_h = (h_new - h) // 2\n",
        "                    start_w = (w_new - w) // 2\n",
        "                    if start_h < 0 or start_w < 0 or start_h + h > h_new or start_w + w > w_new:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = img[start_h:start_h+h, start_w:start_w+w]\n",
        "                else:\n",
        "                    h_new, w_new = img.shape[:2]\n",
        "                    pad_h = (h - h_new) // 2\n",
        "                    pad_w = (w - w_new) // 2\n",
        "                    if pad_h < 0 or pad_w < 0:\n",
        "                        img = cv2.resize(img, (w, h))\n",
        "                    else:\n",
        "                        img = cv2.copyMakeBorder(img, pad_h, h-h_new-pad_h, pad_w, w-w_new-pad_w, cv2.BORDER_CONSTANT)\n",
        "\n",
        "                img = cv2.resize(img, (128, 128))\n",
        "                augmented_images.append(img)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error during augmentation: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return augmented_images\n",
        "\n",
        "    def build_tree_classification_model(self):\n",
        "        \"\"\"Build a transfer learning model for tree type classification\"\"\"\n",
        "        logger.info(\"Building tree classification model\")\n",
        "        try:\n",
        "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(2, activation='softmax')(x)\n",
        "            self.tree_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.tree_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            logger.info(\"Tree classification model built successfully\")\n",
        "            return self.tree_model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building tree classification model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def build_health_analysis_model(self):\n",
        "        \"\"\"Build a model for tree health classification\"\"\"\n",
        "        logger.info(\"Building health analysis model\")\n",
        "        try:\n",
        "            base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "            for layer in base_model.layers:\n",
        "                layer.trainable = False\n",
        "            x = base_model.output\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "            x = Dense(256, activation='relu')(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "            predictions = Dense(4, activation='softmax')(x)\n",
        "            self.health_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "            self.health_model.compile(\n",
        "                optimizer=Adam(learning_rate=0.0001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            logger.info(\"Health analysis model built successfully\")\n",
        "            return self.health_model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error building health analysis model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train_models(self, source_image_path, manual_labels_path=None):\n",
        "        \"\"\"Train both tree classification and health models\"\"\"\n",
        "        logger.info(f\"Training models using source image: {source_image_path}\")\n",
        "        try:\n",
        "            X, y_tree_type = self.create_synthetic_dataset(source_image_path)\n",
        "            X_train, X_val, y_train_tree, y_val_tree = train_test_split(X, y_tree_type, test_size=0.2, random_state=42)\n",
        "            self.build_tree_classification_model()\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "            tree_history = self.tree_model.fit(\n",
        "                X_train, y_train_tree,\n",
        "                validation_data=(X_val, y_val_tree),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "            logger.info(f\"Tree model training complete. Final accuracy: {tree_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "            health_features = self.extract_health_features(X)\n",
        "            y_health = self.generate_synthetic_health_labels(health_features)\n",
        "            _, _, y_train_health, y_val_health = train_test_split(X, y_health, test_size=0.2, random_state=42)\n",
        "            self.build_health_analysis_model()\n",
        "            health_history = self.health_model.fit(\n",
        "                X_train, y_train_health,\n",
        "                validation_data=(X_val, y_val_health),\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=1\n",
        "            )\n",
        "            logger.info(f\"Health model training complete. Final accuracy: {health_history.history['accuracy'][-1]:.4f}\")\n",
        "            return tree_history, health_history\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during model training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_health_features(self, images):\n",
        "        \"\"\"Extract features relevant to tree health\"\"\"\n",
        "        logger.info(\"Extracting health features from images\")\n",
        "        health_features = []\n",
        "        for img in images:\n",
        "            try:\n",
        "                if img.dtype != np.uint8:\n",
        "                    img_uint8 = (img * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    img_uint8 = img\n",
        "                hsv = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2HSV)\n",
        "                avg_h = np.mean(hsv[:, :, 0])\n",
        "                avg_s = np.mean(hsv[:, :, 1])\n",
        "                avg_v = np.mean(hsv[:, :, 2])\n",
        "                green_channel = img_uint8[:, :, 1]\n",
        "                avg_green = np.mean(green_channel)\n",
        "                gray = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2GRAY)\n",
        "                texture = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                features = [avg_h, avg_s, avg_v, avg_green, texture]\n",
        "                health_features.append(features)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting health features: {str(e)}\")\n",
        "                health_features.append([0, 0, 0, 0, 0])\n",
        "        return np.array(health_features)\n",
        "\n",
        "    def generate_synthetic_health_labels(self, features):\n",
        "        \"\"\"Generate synthetic health labels based on features\"\"\"\n",
        "        logger.info(\"Generating synthetic health labels\")\n",
        "        if features.size == 0:\n",
        "            logger.warning(\"Empty features array\")\n",
        "            return np.array([])\n",
        "\n",
        "        features = np.nan_to_num(features)\n",
        "        features_min = features.min(axis=0)\n",
        "        features_max = features.max(axis=0)\n",
        "        range_values = features_max - features_min\n",
        "        range_values[range_values == 0] = 1\n",
        "        normalized_features = (features - features_min) / range_values\n",
        "\n",
        "        green_idx = 3\n",
        "        value_idx = 2\n",
        "        health_labels = []\n",
        "        for sample in normalized_features:\n",
        "            if sample[green_idx] > 0.75 and sample[value_idx] > 0.6:\n",
        "                label = 3  # Healthy\n",
        "            elif sample[green_idx] > 0.5 and sample[value_idx] > 0.5:\n",
        "                label = 2  # Moderate\n",
        "            elif sample[green_idx] > 0.3 and sample[value_idx] > 0.3:\n",
        "                label = 1  # Declining\n",
        "            else:\n",
        "                label = 0  # Needs inspection\n",
        "            health_labels.append(label)\n",
        "\n",
        "        logger.info(f\"Generated {len(health_labels)} health labels\")\n",
        "        return np.array(health_labels)\n",
        "\n",
        "    def process_and_visualize_tif(self, tif_path, output_jpg_path):\n",
        "        \"\"\"Process an entire TIFF file and generate a JPG output with detections\"\"\"\n",
        "        logger.info(f\"Processing TIFF file: {tif_path}\")\n",
        "        try:\n",
        "            if not os.path.exists(tif_path):\n",
        "                logger.error(f\"TIFF file not found: {tif_path}\")\n",
        "                raise FileNotFoundError(f\"TIFF file not found: {tif_path}\")\n",
        "\n",
        "            # Attempt to load the TIFF file using tifffile\n",
        "            try:\n",
        "                full_image = imread(tif_path)\n",
        "                logger.info(f\"Successfully loaded TIFF image using tifffile\")\n",
        "            except ValueError as e:\n",
        "                logger.warning(f\"Failed to load TIFF using tifffile: {str(e)}. Falling back to OpenCV.\")\n",
        "                full_image = cv2.imread(tif_path, cv2.IMREAD_UNCHANGED)\n",
        "                if full_image is None:\n",
        "                    logger.error(f\"Failed to load image using OpenCV: {tif_path}\")\n",
        "                    raise ValueError(f\"Could not read image file: {tif_path}\")\n",
        "\n",
        "            # Convert to a compatible format (e.g., 8-bit RGB or grayscale)\n",
        "            if len(full_image.shape) == 3 and full_image.shape[2] == 3:  # RGB image\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR)\n",
        "            elif len(full_image.shape) == 2:  # Grayscale image\n",
        "                full_image = cv2.cvtColor(full_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            logger.info(f\"Loaded TIFF image with shape: {full_image.shape}\")\n",
        "            results_df = self.process_drone_image_from_array(full_image)\n",
        "            self.generate_detection_visualization(full_image, results_df, output_jpg_path)\n",
        "            logger.info(f\"Detection complete. Output saved to: {output_jpg_path}\")\n",
        "            return results_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing TIFF file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_drone_image_from_array(self, image_array):\n",
        "        \"\"\"Process a full drone image array and classify all trees\"\"\"\n",
        "        logger.info(f\"Processing image array with shape: {image_array.shape}\")\n",
        "        if image_array is None or image_array.size == 0:\n",
        "            logger.error(\"Invalid image array\")\n",
        "            raise ValueError(\"Invalid image array provided\")\n",
        "\n",
        "        tree_locations, tree_images = self.segment_trees_sliding_window(image_array)\n",
        "        if len(tree_locations) == 0:\n",
        "            logger.warning(\"No trees detected. Using fallback method.\")\n",
        "            tree_locations, tree_images = self.create_grid_samples(image_array)\n",
        "        logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "\n",
        "        results = []\n",
        "        batch_size = 32\n",
        "        for i in range(0, len(tree_locations), batch_size):\n",
        "            batch_locations = tree_locations[i:i+batch_size]\n",
        "            batch_images = tree_images[i:i+batch_size]\n",
        "            preprocessed_batch = []\n",
        "            for img in batch_images:\n",
        "                try:\n",
        "                    img_resized = cv2.resize(img, (128, 128))\n",
        "                    img_normalized = img_resized / 255.0\n",
        "                    preprocessed_batch.append(img_normalized)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error preprocessing image {i}: {str(e)}\")\n",
        "                    preprocessed_batch.append(np.zeros((128, 128, 3)))\n",
        "\n",
        "            input_batch = np.array(preprocessed_batch)\n",
        "            if len(input_batch) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                tree_preds = self.tree_model.predict(input_batch)\n",
        "                health_preds = self.health_model.predict(input_batch)\n",
        "                for j, (location, img, tree_pred, health_pred) in enumerate(\n",
        "                    zip(batch_locations, batch_images, tree_preds, health_preds)\n",
        "                ):\n",
        "                    tree_type = \"Palm\" if np.argmax(tree_pred) == 0 else \"Coconut\"\n",
        "                    tree_confidence = float(np.max(tree_pred))\n",
        "                    health_idx = np.argmax(health_pred)\n",
        "                    health_categories = [\"Needs Inspection\", \"Declining Health\", \"Moderate\", \"Healthy\"]\n",
        "                    health_status = health_categories[health_idx]\n",
        "                    health_confidence = float(np.max(health_pred))\n",
        "                    x, y = location\n",
        "                    results.append({\n",
        "                        \"Tree_ID\": i+j+1,\n",
        "                        \"Type\": tree_type,\n",
        "                        \"Type_Confidence\": tree_confidence,\n",
        "                        \"Health\": health_status,\n",
        "                        \"Health_Confidence\": health_confidence,\n",
        "                        \"X_coordinate\": int(x),\n",
        "                        \"Y_coordinate\": int(y)\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during batch prediction: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        results_df = pd.DataFrame(results) if results else pd.DataFrame([{\n",
        "            \"Tree_ID\": 1, \"Type\": \"Palm\", \"Type_Confidence\": 0.8,\n",
        "            \"Health\": \"Healthy\", \"Health_Confidence\": 0.7,\n",
        "            \"X_coordinate\": 100, \"Y_coordinate\": 100\n",
        "        }])\n",
        "        logger.info(f\"Created results dataframe with {len(results_df)} trees\")\n",
        "        return results_df\n",
        "\n",
        "    def generate_detection_visualization(self, image, results_df, output_path):\n",
        "        \"\"\"Generate and save a visualization of detection results as JPG\"\"\"\n",
        "        logger.info(f\"Generating visualization for output: {output_path}\")\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(16, 12))\n",
        "            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            ax.imshow(img_rgb)\n",
        "            health_colors = {\n",
        "                \"Healthy\": \"green\",\n",
        "                \"Moderate\": \"yellowgreen\",\n",
        "                \"Declining Health\": \"orange\",\n",
        "                \"Needs Inspection\": \"red\"\n",
        "            }\n",
        "\n",
        "            for _, row in results_df.iterrows():\n",
        "                x, y = row[\"X_coordinate\"], row[\"Y_coordinate\"]\n",
        "                tree_type = row[\"Type\"]\n",
        "                health_status = row[\"Health\"]\n",
        "                circle = plt.Circle((x, y), 20, color=health_colors[health_status],\n",
        "                                    fill=False, linewidth=2)\n",
        "                ax.add_patch(circle)\n",
        "                label = f\"{tree_type[0]}\\n{health_status[0]}\"\n",
        "                ax.text(x, y, label, color='white', fontsize=8,\n",
        "                        ha='center', va='center', bbox=dict(facecolor=health_colors[health_status],\n",
        "                                                            alpha=0.5))\n",
        "\n",
        "            ax.set_title(\"Tree Detection Results\")\n",
        "            ax.axis('off')\n",
        "            plt.savefig(output_path, format='jpg', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            logger.info(f\"Visualization saved to {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating visualization: {str(e)}\")\n",
        "            blank_img = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
        "            cv2.imwrite(output_path, blank_img)\n",
        "\n",
        "    def create_grid_samples(self, image, grid_size=50):\n",
        "        \"\"\"Create a grid of sample points as a fallback method\"\"\"\n",
        "        logger.info(\"Creating grid samples as fallback\")\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "        for y in range(grid_size, h-grid_size, grid_size):\n",
        "            for x in range(grid_size, w-grid_size, grid_size):\n",
        "                patch = image[y-grid_size//2:y+grid_size//2, x-grid_size//2:x+grid_size//2]\n",
        "                hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "                lower_green = np.array([30, 40, 40])\n",
        "                upper_green = np.array([90, 255, 255])\n",
        "                mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "                if np.sum(mask) > (grid_size * grid_size * 0.1):\n",
        "                    tree_locations.append((x, y))\n",
        "                    tree_images.append(patch)\n",
        "        logger.info(f\"Created {len(tree_locations)} grid samples\")\n",
        "        return tree_locations, tree_images\n",
        "\n",
        "    def segment_trees_sliding_window(self, image, window_size=128, stride=64):\n",
        "        \"\"\"Segment trees using a sliding window approach\"\"\"\n",
        "        logger.info(\"Segmenting trees using sliding window approach\")\n",
        "        if image is None or image.shape[0] == 0 or image.shape[1] == 0:\n",
        "            logger.error(\"Cannot segment trees: Invalid image\")\n",
        "            return [], []\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        tree_locations = []\n",
        "        tree_images = []\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "            lower_green = np.array([30, 40, 40])\n",
        "            upper_green = np.array([90, 255, 255])\n",
        "            veg_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "            for y in range(0, h-window_size, stride):\n",
        "                for x in range(0, w-window_size, stride):\n",
        "                    window = image[y:y+window_size, x:x+window_size]\n",
        "                    mask_window = veg_mask[y:y+window_size, x:x+window_size]\n",
        "                    veg_ratio = np.sum(mask_window > 0) / (window_size * window_size)\n",
        "\n",
        "                    if veg_ratio > 0.3:\n",
        "                        mask_copy = mask_window.copy()\n",
        "                        contours, _ = cv2.findContours(mask_copy, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                        for contour in contours:\n",
        "                            area = cv2.contourArea(contour)\n",
        "                            if area > 100:\n",
        "                                perimeter = cv2.arcLength(contour, True)\n",
        "                                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
        "\n",
        "                                if circularity > 0.4:\n",
        "                                    M = cv2.moments(contour)\n",
        "                                    if M[\"m00\"] > 0:\n",
        "                                        c_x = int(M[\"m10\"] / M[\"m00\"]) + x\n",
        "                                        c_y = int(M[\"m01\"] / M[\"m00\"]) + y\n",
        "                                        tree_x1 = max(0, c_x - window_size//2)\n",
        "                                        tree_y1 = max(0, c_y - window_size//2)\n",
        "                                        tree_x2 = min(w, c_x + window_size//2)\n",
        "                                        tree_y2 = min(h, c_y + window_size//2)\n",
        "                                        tree_img = image[tree_y1:tree_y2, tree_x1:tree_x2]\n",
        "\n",
        "                                        if tree_img.shape[0] > 0 and tree_img.shape[1] > 0:\n",
        "                                            tree_img_resized = cv2.resize(tree_img, (128, 128))\n",
        "                                            tree_images.append(tree_img_resized)\n",
        "                                            tree_locations.append((c_x, c_y))\n",
        "\n",
        "            logger.info(f\"Detected {len(tree_locations)} potential trees\")\n",
        "            return tree_locations, tree_images\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in tree segmentation: {str(e)}\")\n",
        "            return [], []\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = PalmTreeAnalysis()\n",
        "\n",
        "    # Training\n",
        "    source_image = \"4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    analyzer.train_models(source_image)\n",
        "\n",
        "    # Testing with TIFF file\n",
        "    tif_file = \"4562d4b9-3ebd-4d73-a6e3-9a713a9fa608.tif\"\n",
        "    output_jpg = \"path/to/output_detections.jpg\"\n",
        "    results = analyzer.process_and_visualize_tif(tif_file, output_jpg)\n",
        "    print(results)"
      ],
      "metadata": {
        "id": "f_5rSeXeGqGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "kMNNxVcuHghI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MASTER_ADDR']='localhost'\n",
        "os.environ['MASTER_PORT']='12345'\n",
        "os.environ['WORLD_SIZE']=str(4)\n",
        "os.environ['RANK']=str(0)\n",
        "dist.init_process_group(backend='nccl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "wwcNPSg3LkT4",
        "outputId": "3e0b57b5-be93-45b6-aee9-8395f0e8bfe0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DistStoreError",
          "evalue": "Timed out after 601 seconds waiting for clients. 1/4 clients joined.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDistStoreError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-426f830629d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WORLD_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RANK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nccl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmsg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_msg_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mfunc_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mnot_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m             )\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendezvous_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0muse_libuv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_use_libuv_from_query_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     store = _create_c10d_store(\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mmaster_addr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_libuv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_create_c10d_store\u001b[0;34m(hostname, port, rank, world_size, timeout, use_libuv)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mstart_daemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return TCPStore(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mhostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDistStoreError\u001b[0m: Timed out after 601 seconds waiting for clients. 1/4 clients joined."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.fsdp import FullyShardedDataParallel as fsdp\n",
        "\n",
        "model=model()\n",
        "model=fsdp(model)"
      ],
      "metadata": {
        "id": "UPMPtM2MLwPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader DistributedSampler"
      ],
      "metadata": {
        "id": "cOxRCdrVNq85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=sampler()\n",
        "sampler=DistributedSampler(dataset)\n",
        "dataloader=DataLoader(dataset,sampler=sampler,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oMV-Frg-Nq_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=<rank_of_this_node> your_training_script.py"
      ],
      "metadata": {
        "id": "C0VSUnnPNrCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}